# Exploring LLM Cognition ability to explain Chess Moves

## Overview
The project aims to understand LLM's cognition ability to explain chess moves. It utilizes data generated via the OpenAI API and employs advanced techniques such as prompt engineering, LoRA, model quantization etc.

## Dataset
The dataset for this project was generated via the OpenAI API. Initially sourced from the Lichess dataset, each game was converted into progressive moves mimicking a typical NLP training dataset, with explanations generated using "GPT-3.5-turbo". However, upon inspection, it was found that the explanations generated were not consistently accurate. This led to the adoption of prompt engineering techniques to improve the quality of explanations.  
Additionally I used a custom dataset which were generated by gpt 4 and verified by 2 other team members. Unfortunately the explanations' structure was not consistent with the AI generated dataset, also it used the piece name "castle" and "rook" interchangeably. So I ended up not using the datset much during training.  

## Prompt Engineering
Prompt engineering for this task is complicated and involves -  
- breaking down the task into manageable sets
- analyzing the board state after each move
- determining which turn white or black
- get the last move from the list of moves
- attempting to explain the rationale behind the last move.   
  
Due to the **lack of persistence** in OpenAI API responses, previous findings had to be fed into subsequent requests to ensure accurate explanation generation.   
This approach would significantly increase the number of tokens used and incur higher costs, leading to the optimization of the process. Besides, OpenAI has its own daily limitation on token and API requests.  
Hence I incorporated **last move for each of my requests**. This immediately improved the explanation quality and I think incorporating whose turn it is during the "last move" would make the explanations even better.

## Models Used
Several models were utilized in this project, including Falcon 7B, Gemma 7B, and 2B. For fine-tuning, LoRA and QLoRA were used.

- **LoRA**: Utilizes matrix decomposition to generate the adapters for fine-tuning, while freezing the rest of the weights. This technique optimizes memory usage during training. The base model is stored in 16 bit precision, the gradient calculation will occur in 32 bits and adapters will be in 16 bits.
- **QLoRA**: Takes hardware usage efficiency further by implementing **lower- 4-bit precision**, for enhanced memory and computational efficiency. The base model is stored in 4 bit precision that limits the model memory usage footprint.

### Falcon Model
- Utilized the falcon-7b-instruct model, with the LoRA target module/weight matrix being "query_key_value".
- Implemented LoRA to select only the necessary parameters for training, reducing memory consumption. Using LoRA we can get something like the following - 
```
trainable params: 4718592 || all params: 3613463424 || trainables%: 0.13058363808693696
```
- The base model is stored in 16-bit precision ~ 15 gigs, but after conversion with QLoRA, it occupies approximately 4 gigs of space for inference.
- **Please suggest if incorrect** - The base model is stored in 16 bit precision, which explains the memory consumption -  
```
15gigs × 8 bit × 1024 MB × 1024KB × 1024 Bytes /16 bit precison ~ 8 billion parameters 
```
  
![Falcon](https://github.com/Adnan525/cognitive_AI/blob/master/falcon7b_loading.png)

### Gemma Model
- Instead of using the transformers module, I employed Unsloth for Gemma model loading.
- Unsloth loads the 4-bit quantized model directly, enabling significantly faster loading times.
- With the combination of FastLanguage, LoRA, QLoRA's 4-bit precision, and Unsloth's fastattention, rewritten several transformer modules, RoPE embedding, a faster cross-entropy loss function, reduced data up-casting during inference, and utilization of bfloat16 the Gemma model trained faster compared to Falcon.
- Additionally I tested several 2B models which performed somewhat well considering it's a proof of concept but during my test, they definitely did not have same precision as the 7B model. For example -  the gemma 7B model trained on AI generated dataset successfully explained a move where Black Queen changed position to guard a Bishop. On other hand the 2B model had a vague explanation suggesting the move was performed to control the central squares.
  
![Gemma](https://github.com/Adnan525/cognitive_AI/blob/master/gemm7b_loading.png)

## Challenges and Future Directions
While both Falcon and Gemma models showed promising outputs, they often suffered from hallucinations and repeated patterns due to the unrefined dataset like "central square", "develop pawn structure", "develop attack on king/queen side" etc. Future work involves refining the dataset and exploring additional techniques to improve model performance and understanding of chess moves.

## References
- [Chain-of-Thought Prompting Elicits Reasoning
in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
- [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)
- [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
- [QLoRA - Medium](https://medium.com/@ayyucedemirbas/qlora-4444944c20bd)
- [Memory-efficient Fine-tuning with with QLoRA - Andreas Nauerz](https://heidloff.net/article/qlora/)
- [4-bit Transformers - Hugging Face](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- [Unsloth - Kaitchup](https://kaitchup.substack.com/p/unsloth-faster-and-memory-efficient)
- [Finetuning with Unsloth and QLoRA - Hugging Face Blog](https://huggingface.co/blog/Andyrasika/finetune-unsloth-qlora)

